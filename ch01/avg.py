import numpy as np

# このコードは「ランダムに得られる報酬 reward の平均（期待値の推定）」を、
# 2通りの方法で逐次計算していく例です。
#
# 強化学習（RL）では、状態価値 V(s) や行動価値 Q(s,a) を
# 「サンプルで得られたリターン（return）や報酬の平均」として推定することが多く、
# その最も基本形が「標本平均（sample average）」です。
#
# ここでは reward を np.random.rand()（[0,1) の一様乱数）で生成しているので、
# 真の期待値 E[reward] は 0.5 です（理論上）。
# ただしサンプル数が少ないうちは推定値 Q は揺れ、サンプルが増えると 0.5 に近づきます。

# ------------------------------------------------------------
# naive implementation（素朴な実装）
# ------------------------------------------------------------
# 目的：n 個目までに観測した reward の平均値 Q_n を毎回「全部足して割る」で計算する。
# 計算量：各ステップで sum(rewards) が O(n) なので全体で O(n^2) になりがち。
# メモリ：rewards に全履歴を保持するので O(n)。
np.random.seed(0)  # 乱数の再現性確保：同じ seed なら同じ乱数列が生成される
rewards = []  # 観測した報酬を全て保存（後で合計するため）

for n in range(1, 11):
    # 1ステップで得られた報酬サンプル（RLでいう r_{t+1} 的なもの）
    reward = np.random.rand()
    rewards.append(reward)

    # n 個の標本平均：
    # Q_n = (r_1 + r_2 + ... + r_n) / n
    Q = sum(rewards) / n

    print(Q)

print("---")

# ------------------------------------------------------------
# incremental implementation（逐次更新の実装）
# ------------------------------------------------------------
# 目的：標本平均を「履歴を保存せず」に O(1) メモリで更新する。
# 強化学習の価値推定（バンディットの平均報酬推定、モンテカルロ法の平均など）で重要。
#
# 導出（重要）：
#   Q_{n-1} = (r_1 + ... + r_{n-1}) / (n-1)
#   Q_n     = (r_1 + ... + r_{n-1} + r_n) / n
#          = ((n-1)/n) * Q_{n-1} + (1/n) * r_n
#          = Q_{n-1} + (r_n - Q_{n-1}) / n
#
# つまり「新サンプル r_n と現在推定 Q の誤差（r_n - Q）を 1/n だけ反映」すればよい。
# この更新は RL の基本形：
#   推定値 ← 推定値 + 学習率 * (ターゲット - 推定値)
# の「誤差駆動更新（error-driven update）」の最も単純な形だと見なせる。
#
# ここでの学習率は α_n = 1/n（サンプル数に応じて減衰）で、
# これを使うと逐次更新しても「厳密に標本平均」と一致する。
np.random.seed(0)  # 上と同じ乱数列にする（両者が同じ出力になることを確認できる）
Q = 0  # 推定値の初期化（n=1 の更新で Q は r_1 に一致する）

for n in range(1, 11):
    reward = np.random.rand()

    # 逐次更新：
    # Q ← Q + (reward - Q) / n
    #
    # (reward - Q) は「予測誤差」に相当する。
    # 1/n で割ることで、n 個の標本の平均にちょうど一致する重み付けになる。
    #
    # 注意（RLでよくある話）：
    # - 非定常環境（平均報酬が時間で変わる）では 1/n は小さくなりすぎて追従しにくい。
    #   その場合は固定学習率 α（例 0.1）で指数移動平均にするのが典型。
    Q = Q + (reward - Q) / n

    print(Q)
