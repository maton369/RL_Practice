# このコードは「ベルマン期待方程式（Bellman expectation equation）」を
# 反復的に解くことで、状態価値関数 V(s) を数値的に求める例です。
#
# 強化学習（特に動的計画法 / Dynamic Programming）では、
# 方策 π が固定されているときの状態価値 V^π(s) は
# ベルマン方程式の連立方程式として書けます。
#
# 価値反復（Value Iteration）と名前が似ていますが、ここでやっているのは
# 「方策評価（Policy Evaluation）」に相当します。
# つまり「ある固定方策（ここでは各状態で 2つの行動を 0.5 ずつ選ぶ）」の下での
# V^π を反復更新で収束させています。
#
# なお、2状態（L1, L2）しかないので、本来は連立一次方程式として厳密に解けますが、
# 一般の大規模問題に拡張可能な「反復解法（Iterative Policy Evaluation）」として実装しているのがポイントです。

# -------------------------
# 価値関数の初期化
# -------------------------
# V は現在の価値推定値 V_k(s) を格納する辞書。
# 初期値は 0 から開始（よくある初期化）。
V = {"L1": 0.0, "L2": 0.0}

# new_V は「次の反復で更新された価値」V_{k+1}(s) を格納するためのバッファ。
# こうしてバッファを分けることで、更新が「同期更新（synchronous update）」になる。
# もし V をその場で更新すると「非同期更新（asynchronous update）」になり、収束挙動が変わり得る。
new_V = V.copy()

# 反復回数のカウント
cnt = 0

# -------------------------
# 反復方策評価（Iterative Policy Evaluation）
# -------------------------
while True:
    # discount factor（割引率）γ = 0.9
    # γ は「将来の報酬をどれだけ重視するか」を表すパラメータ。
    #
    # 価値関数の定義（方策 π の下）：
    #
    #   V^π(s) = E_π [ Σ_{t=0}^∞ γ^t R_{t+1} | S_0 = s ]
    #
    # ここでは「次状態の価値」に 0.9 を掛けている部分が γ を表している。

    # -------------------------
    # L1 のベルマン期待更新
    # -------------------------
    # new_V['L1'] は次の形の期待値計算になっている：
    #
    #   V_{k+1}(L1) = Σ_a π(a|L1) Σ_{s'} P(s'|L1,a) [ r(L1,a,s') + γ V_k(s') ]
    #
    # このコードでは L1 で取りうる行動が2つあり、方策 π が「0.5ずつ」選ぶと仮定されている。
    # そのため「0.5 * (...) + 0.5 * (...)」という形になっている。
    #
    # 1つ目の分岐：即時報酬 -1 を得て、次状態が L1 になる（と解釈できる）
    #   -> -1 + 0.9 * V['L1']
    #
    # 2つ目の分岐：即時報酬 +1 を得て、次状態が L2 になる（と解釈できる）
    #   ->  1 + 0.9 * V['L2']
    #
    # つまり
    #   V_{k+1}(L1) = 0.5 * (-1 + γ V_k(L1)) + 0.5 * ( 1 + γ V_k(L2))
    #
    # というベルマン更新をそのまま書いている。
    new_V["L1"] = 0.5 * (-1 + 0.9 * V["L1"]) + 0.5 * (1 + 0.9 * V["L2"])

    # -------------------------
    # L2 のベルマン期待更新
    # -------------------------
    # 同様に L2 でも行動が2つあり 0.5 ずつ選ぶと仮定されている。
    #
    # 1つ目の分岐：即時報酬 0 を得て、次状態が L1
    #   -> 0 + 0.9 * V['L1']
    #
    # 2つ目の分岐：即時報酬 -1 を得て、次状態が L2
    #   -> -1 + 0.9 * V['L2']
    #
    # よって
    #   V_{k+1}(L2) = 0.5 * (0 + γ V_k(L1)) + 0.5 * (-1 + γ V_k(L2))
    #
    # となる。
    new_V["L2"] = 0.5 * (0 + 0.9 * V["L1"]) + 0.5 * (-1 + 0.9 * V["L2"])

    # -------------------------
    # 収束判定（最大ノルム / sup norm）
    # -------------------------
    # delta は今回の更新でどれだけ価値が変わったか（誤差）を表す。
    # ここでは max_s |V_{k+1}(s) - V_k(s)| を計算している。
    #
    # 価値反復・方策評価でよく使われる収束判定で、
    # delta が小さくなるほど「ベルマン方程式の固定点」に近づいたとみなせる。
    delta = abs(new_V["L1"] - V["L1"])
    delta = max(delta, abs(new_V["L2"] - V["L2"]))

    # 次反復のために V <- new_V として更新（同期更新）
    V = new_V.copy()

    # 反復回数を増やす
    cnt += 1

    # -------------------------
    # 収束したら終了
    # -------------------------
    # delta < 1e-4 なら十分収束したとみなす（閾値は任意で調整可能）。
    # γ<1 のとき、ベルマン更新は収縮写像（contraction）になりやすく、
    # 反復によって一意な固定点 V^π へ収束することが期待される。
    if delta < 0.0001:
        # 収束した価値推定（固定方策 π の下の V^π の近似）
        print(V)
        # 収束までに必要だった反復回数
        print(cnt)
        break
