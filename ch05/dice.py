import numpy as np

# このコードは「モンテカルロ推定（Monte Carlo Estimation）」の最小例で、
# サイコロを振って得られる確率変数 X の期待値 E[X] を
# サンプル平均で推定していく過程を実装しています。
#
# 強化学習（RL）との関係：
# - モンテカルロ法は「エピソードをサンプルして、得られたリターン（return）の平均から
#   価値 V(s) や Q(s,a) を推定する」枠組みとして登場します。
# - ここではまだ状態や方策は出てきませんが、やっていることは
#   「観測したサンプルの平均で期待値を近似する」という MC の核心そのものです。


def sample(dices=2):
    """
    サイコロを dices 回振って出目を合計し、その合計値を返す関数。

    - 1回の試行で得られる s が確率変数 X の1サンプルになる。
    - dices=2 のとき、X は「2個の独立な公平サイコロの和」。

    理論：
    各サイコロは一様分布 {1,2,3,4,5,6} を持つので、1個の期待値は

        E[die] = (1+2+3+4+5+6) / 6 = 21/6 = 3.5

    独立な2個の和なら線形性より

        E[X] = E[die1 + die2] = E[die1] + E[die2] = 3.5 + 3.5 = 7.0

    したがって、十分多く試行すれば推定値 V は 7 に近づくはず。
    """
    x = 0
    for _ in range(dices):
        # np.random.choice で {1..6} から一様に1つ選ぶ
        # （公平なサイコロの出目を模したもの）
        x += np.random.choice([1, 2, 3, 4, 5, 6])
    return x


# 試行回数（サンプル数）
trial = 1000

# V: 期待値の推定値（running estimate）
# n: 観測したサンプル数
#
# 初期値 V=0 は任意でよい（最初の更新で一気にサンプル値に近づく）
V, n = 0, 0

for _ in range(trial):
    # 1試行：確率変数 X のサンプルを1つ生成
    s = sample()

    # サンプル数をインクリメント
    n += 1

    # インクリメンタルなサンプル平均更新（逐次平均）
    #
    # 逐次平均の一般形：
    #   V_n = (1/n) * Σ_{i=1..n} s_i
    #
    # これを「前回までの平均 V_{n-1}」を使って更新すると
    #   V_n = V_{n-1} + (s_n - V_{n-1}) / n
    #
    # となる。これがこの1行の更新式。
    #
    # メリット：
    # - 全サンプルを保存せずに、O(1) メモリで平均を更新できる
    # - RLの価値推定でも同じ形が頻出する（例：MC推定の平均、バンディットのサンプル平均更新）
    #
    # 解釈：
    # - (s - V) は「予測誤差（error）」。
    # - 1/n は「学習率」に相当し、サンプルが増えるほど新しい観測の影響が小さくなる。
    V += (s - V) / n

    # 現在の推定値（サンプル平均）を表示
    # n が増えるにつれ、値は 7.0 付近に安定していくことが期待される。
    print(V)
