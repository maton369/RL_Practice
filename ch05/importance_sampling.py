import numpy as np

# このコードは「期待値の計算」と「モンテカルロ推定」、さらに
# 「重要度サンプリング（Importance Sampling; IS）」による期待値推定を
# 1つの最小例で比較するものです。
#
# 強化学習（RL）との関係：
# - MC（モンテカルロ）は、方策 π の下で得られたサンプルから期待値（価値）を推定する基本手法。
# - 重要度サンプリングは、別の分布（行動方策 b）で集めたデータから、
#   目的の分布（ターゲット方策 π）の期待値を推定するための道具。
#   これは「オフポリシー評価（off-policy evaluation）」の基礎に直結します。

# 値を取り得る確率変数 X の取り得る値（離散的なサポート）
# ここでは X ∈ {1,2,3}
x = np.array([1, 2, 3])

# ターゲット分布（ターゲット方策） π
# X=1 を 0.1、X=2 を 0.1、X=3 を 0.8 の確率で出す分布
# RLの文脈では「評価したい方策 π が誘導する分布」と捉えられる
pi = np.array([0.1, 0.1, 0.8])

# ============================================================
# 1) Expectation（厳密な期待値計算）
# ============================================================
# 離散分布でサポートが小さい場合は期待値は和で厳密に計算できる：
#
#   E_pi[X] = Σ_x x * π(x)
#
# ここで x と pi は同じインデックス対応なので、要素積 x*pi を総和すればよい。
e = np.sum(x * pi)
print("E_pi[x]", e)

# ============================================================
# 2) Monte Carlo（ターゲット分布 π から直接サンプルして推定）
# ============================================================
# モンテカルロ推定は「サンプル平均で期待値を近似する」方法。
#
#   X_1,...,X_n ~ π を独立にサンプルすると
#
#   E_pi[X] ≈ (1/n) Σ_i X_i
#
# 大数の法則により n→∞ で真の期待値に収束する。
#
# ただし有限 n ではばらつくので、推定値の分散（サンプル分散）を確認する。
n = 100
samples = []
for _ in range(n):
    # π に従って {1,2,3} からサンプルを生成
    s = np.random.choice(x, p=pi)
    samples.append(s)

# 推定値：平均、ばらつき：分散
print("MC: {:.2f} (var: {:.2f})".format(np.mean(samples), np.var(samples)))

# ============================================================
# 3) Importance Sampling（別分布 b で集めたデータで E_pi[X] を推定）
# ============================================================
# 重要度サンプリング（IS）は「別の分布 b からサンプルしたのに、
# 目的の分布 π の期待値を推定したい」という状況で使う。
#
# 基本式（離散でも連続でも同じ構造）：
#
#   E_pi[f(X)] = Σ_x f(x) π(x)
#              = Σ_x f(x) b(x) * (π(x)/b(x))
#              = E_b[ ρ(X) f(X) ]
#
# ここで
#   ρ(x) = π(x) / b(x)
# が重要度比（importance ratio）。
#
# つまり「bで出にくいがπでは出やすいサンプル」は重みが大きくなり、
# 逆に「bで出やすいがπでは出にくいサンプル」は重みが小さくなる。
#
# RLでは：
# - b：行動方策（behavior policy）＝データを集めた方策
# - π：ターゲット方策（target policy）＝評価したい方策
# であり、オフポリシー評価の根幹になる。
#
# 注意（成立条件）：
# - b(x) = 0 なのに π(x) > 0 の点があると ρ が無限大になり推定不能。
#   したがって「π のサポートは b のサポートに含まれる」必要がある。
#   （support(π) ⊆ support(b)）
#
# ここでは b を π に近い分布にして、重みが暴れすぎないようにしている。
b = np.array([0.2, 0.2, 0.6])  # 例：一様分布 b=[1/3,1/3,1/3] でもOK

samples = []
for _ in range(n):
    # b でサンプルするのは「値そのもの」ではなくインデックスでも良い
    # idx = [0,1,2] を b に従って選び、そのインデックスに対応する値 x[i] を観測する
    idx = np.arange(len(b))  # [0, 1, 2]
    i = np.random.choice(idx, p=b)
    s = x[i]

    # 重要度比 ρ = π(x)/b(x)
    # 離散分布なので「そのサンプルが出た点の確率比」を取れば良い
    rho = pi[i] / b[i]

    # IS推定量（普通のIS：weighted return / not normalized）
    # f(X)=X として
    #   推定サンプル = ρ * s
    # を平均することで E_pi[X] を推定する。
    #
    # 注意：
    # - これは「通常のIS（ordinary IS）」で、不偏（期待値は正しい）だが分散が大きくなり得る。
    # - RLでは分散を抑えるため「自己正規化IS（weighted IS）」を使うことも多い：
    #
    #     E_pi[X] ≈ Σ_i ρ_i s_i / Σ_i ρ_i
    #
    #   こちらは一般に分散は下がりやすいが、わずかにバイアスが入る。
    samples.append(rho * s)

print("IS: {:.2f} (var: {:.2f})".format(np.mean(samples), np.var(samples)))
